{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T18:22:48.474113Z",
     "start_time": "2022-02-02T18:22:34.999363Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import swifter\n",
    "import nltk\n",
    "import string\n",
    "from typing import List\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-30T09:24:39.913547Z",
     "start_time": "2021-12-30T09:24:39.900579Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(path: str) -> pd.DataFrame:\n",
    "    rows_list = []\n",
    "    _, authors, _ = next(os.walk(path))\n",
    "    for author_name in authors:\n",
    "        curr_row = {\"author_name\": author_name}\n",
    "        author_path = os.path.join(path, author_name)\n",
    "        _, _, books_files = next(os.walk(author_path))\n",
    "        for book_name in books_files:\n",
    "            curr_row[\"book_name\"] = book_name\n",
    "            with open(os.path.join(author_path, book_name), \"r\") as book:\n",
    "                curr_row[\"book_text\"] = book.read()\n",
    "            rows_list.append(curr_row.copy())\n",
    "    return pd.DataFrame(rows_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-30T09:24:39.929505Z",
     "start_time": "2021-12-30T09:24:39.917537Z"
    }
   },
   "outputs": [],
   "source": [
    "def chunck(df: pd.DataFrame, chunck_size: int = 500, text_colum: str = 'book_text') -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        num_chuncks = len(row[text_colum]) // chunck_size\n",
    "        for i in range(num_chuncks - 1):\n",
    "            tmp_row = row.copy()\n",
    "            tmp_row[text_colum] = tmp_row[text_colum][i*chunck_size : (i+1)*chunck_size]\n",
    "            rows.append(tmp_row.copy())\n",
    "        tmp_row = row.copy()\n",
    "        tmp_row[text_colum] = tmp_row[text_colum][(num_chuncks - 1)*chunck_size : ]\n",
    "        rows.append(tmp_row.copy())\n",
    "    return pd.DataFrame(rows)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-30T09:24:53.371058Z",
     "start_time": "2021-12-30T09:24:39.937483Z"
    }
   },
   "outputs": [],
   "source": [
    "df = load_data('C50/C50train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-30T09:24:53.401979Z",
     "start_time": "2021-12-30T09:24:53.374055Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_name</th>\n",
       "      <th>book_name</th>\n",
       "      <th>book_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>106247newsML.txt</td>\n",
       "      <td>The Internet may be overflowing with new techn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>120600newsML.txt</td>\n",
       "      <td>The U.S. Postal Service announced Wednesday a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>120683newsML.txt</td>\n",
       "      <td>Elementary school students with access to the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>136958newsML.txt</td>\n",
       "      <td>An influential Internet organisation has backe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>137498newsML.txt</td>\n",
       "      <td>An influential Internet organisation has backe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>WilliamKazer</td>\n",
       "      <td>28223newsML.txt</td>\n",
       "      <td>China's central bank chief has said that infla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>WilliamKazer</td>\n",
       "      <td>282935newsML.txt</td>\n",
       "      <td>China ushered in 1997, a year it has hailed as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>WilliamKazer</td>\n",
       "      <td>287736newsML.txt</td>\n",
       "      <td>China issued tough new rules on the handling o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>WilliamKazer</td>\n",
       "      <td>289747newsML.txt</td>\n",
       "      <td>China will avoid bold moves in tackling its ai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>WilliamKazer</td>\n",
       "      <td>304402newsML.txt</td>\n",
       "      <td>Communist Party chief Jiang Zemin has put his ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        author_name         book_name  \\\n",
       "0     AaronPressman  106247newsML.txt   \n",
       "1     AaronPressman  120600newsML.txt   \n",
       "2     AaronPressman  120683newsML.txt   \n",
       "3     AaronPressman  136958newsML.txt   \n",
       "4     AaronPressman  137498newsML.txt   \n",
       "...             ...               ...   \n",
       "2495   WilliamKazer   28223newsML.txt   \n",
       "2496   WilliamKazer  282935newsML.txt   \n",
       "2497   WilliamKazer  287736newsML.txt   \n",
       "2498   WilliamKazer  289747newsML.txt   \n",
       "2499   WilliamKazer  304402newsML.txt   \n",
       "\n",
       "                                              book_text  \n",
       "0     The Internet may be overflowing with new techn...  \n",
       "1     The U.S. Postal Service announced Wednesday a ...  \n",
       "2     Elementary school students with access to the ...  \n",
       "3     An influential Internet organisation has backe...  \n",
       "4     An influential Internet organisation has backe...  \n",
       "...                                                 ...  \n",
       "2495  China's central bank chief has said that infla...  \n",
       "2496  China ushered in 1997, a year it has hailed as...  \n",
       "2497  China issued tough new rules on the handling o...  \n",
       "2498  China will avoid bold moves in tackling its ai...  \n",
       "2499  Communist Party chief Jiang Zemin has put his ...  \n",
       "\n",
       "[2500 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-30T09:24:56.414973Z",
     "start_time": "2021-12-30T09:24:53.404972Z"
    }
   },
   "outputs": [],
   "source": [
    "df2 = chunck(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-30T09:24:56.430330Z",
     "start_time": "2021-12-30T09:24:56.416028Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_name</th>\n",
       "      <th>book_name</th>\n",
       "      <th>book_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>106247newsML.txt</td>\n",
       "      <td>The Internet may be overflowing with new techn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>106247newsML.txt</td>\n",
       "      <td>te, which collects reports directly from consu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>106247newsML.txt</td>\n",
       "      <td>over $6 million, promising investors they cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>106247newsML.txt</td>\n",
       "      <td>omputer equipment, such as memory chips or sou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>120600newsML.txt</td>\n",
       "      <td>The U.S. Postal Service announced Wednesday a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>WilliamKazer</td>\n",
       "      <td>304402newsML.txt</td>\n",
       "      <td>prosperous.\\nState television estimated 224 mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>WilliamKazer</td>\n",
       "      <td>304402newsML.txt</td>\n",
       "      <td>turned a backward Stalinist state into an eco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>WilliamKazer</td>\n",
       "      <td>304402newsML.txt</td>\n",
       "      <td>the drama from an otherwise predictable serie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>WilliamKazer</td>\n",
       "      <td>304402newsML.txt</td>\n",
       "      <td>returns Hong Kong to China this year.\\nThe la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>WilliamKazer</td>\n",
       "      <td>304402newsML.txt</td>\n",
       "      <td>give their own assessment of the man who is p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14145 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        author_name         book_name  \\\n",
       "0     AaronPressman  106247newsML.txt   \n",
       "0     AaronPressman  106247newsML.txt   \n",
       "0     AaronPressman  106247newsML.txt   \n",
       "0     AaronPressman  106247newsML.txt   \n",
       "1     AaronPressman  120600newsML.txt   \n",
       "...             ...               ...   \n",
       "2499   WilliamKazer  304402newsML.txt   \n",
       "2499   WilliamKazer  304402newsML.txt   \n",
       "2499   WilliamKazer  304402newsML.txt   \n",
       "2499   WilliamKazer  304402newsML.txt   \n",
       "2499   WilliamKazer  304402newsML.txt   \n",
       "\n",
       "                                              book_text  \n",
       "0     The Internet may be overflowing with new techn...  \n",
       "0     te, which collects reports directly from consu...  \n",
       "0      over $6 million, promising investors they cou...  \n",
       "0     omputer equipment, such as memory chips or sou...  \n",
       "1     The U.S. Postal Service announced Wednesday a ...  \n",
       "...                                                 ...  \n",
       "2499  prosperous.\\nState television estimated 224 mi...  \n",
       "2499   turned a backward Stalinist state into an eco...  \n",
       "2499   the drama from an otherwise predictable serie...  \n",
       "2499   returns Hong Kong to China this year.\\nThe la...  \n",
       "2499   give their own assessment of the man who is p...  \n",
       "\n",
       "[14145 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-30T09:24:56.446289Z",
     "start_time": "2021-12-30T09:24:56.433323Z"
    }
   },
   "outputs": [],
   "source": [
    "df2['author_name'] = pd.factorize(df2['author_name'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-30T09:24:56.462276Z",
     "start_time": "2021-12-30T09:24:56.448282Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_name</th>\n",
       "      <th>book_name</th>\n",
       "      <th>book_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>106247newsML.txt</td>\n",
       "      <td>The Internet may be overflowing with new techn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>106247newsML.txt</td>\n",
       "      <td>te, which collects reports directly from consu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>106247newsML.txt</td>\n",
       "      <td>over $6 million, promising investors they cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>106247newsML.txt</td>\n",
       "      <td>omputer equipment, such as memory chips or sou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>120600newsML.txt</td>\n",
       "      <td>The U.S. Postal Service announced Wednesday a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>49</td>\n",
       "      <td>304402newsML.txt</td>\n",
       "      <td>prosperous.\\nState television estimated 224 mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>49</td>\n",
       "      <td>304402newsML.txt</td>\n",
       "      <td>turned a backward Stalinist state into an eco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>49</td>\n",
       "      <td>304402newsML.txt</td>\n",
       "      <td>the drama from an otherwise predictable serie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>49</td>\n",
       "      <td>304402newsML.txt</td>\n",
       "      <td>returns Hong Kong to China this year.\\nThe la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>49</td>\n",
       "      <td>304402newsML.txt</td>\n",
       "      <td>give their own assessment of the man who is p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14145 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      author_name         book_name  \\\n",
       "0               0  106247newsML.txt   \n",
       "0               0  106247newsML.txt   \n",
       "0               0  106247newsML.txt   \n",
       "0               0  106247newsML.txt   \n",
       "1               0  120600newsML.txt   \n",
       "...           ...               ...   \n",
       "2499           49  304402newsML.txt   \n",
       "2499           49  304402newsML.txt   \n",
       "2499           49  304402newsML.txt   \n",
       "2499           49  304402newsML.txt   \n",
       "2499           49  304402newsML.txt   \n",
       "\n",
       "                                              book_text  \n",
       "0     The Internet may be overflowing with new techn...  \n",
       "0     te, which collects reports directly from consu...  \n",
       "0      over $6 million, promising investors they cou...  \n",
       "0     omputer equipment, such as memory chips or sou...  \n",
       "1     The U.S. Postal Service announced Wednesday a ...  \n",
       "...                                                 ...  \n",
       "2499  prosperous.\\nState television estimated 224 mi...  \n",
       "2499   turned a backward Stalinist state into an eco...  \n",
       "2499   the drama from an otherwise predictable serie...  \n",
       "2499   returns Hong Kong to China this year.\\nThe la...  \n",
       "2499   give their own assessment of the man who is p...  \n",
       "\n",
       "[14145 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-30T09:24:56.478205Z",
     "start_time": "2021-12-30T09:24:56.464239Z"
    }
   },
   "outputs": [],
   "source": [
    "X, Y = pd.DataFrame(df2['book_text']), df2['author_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-30T09:24:56.494161Z",
     "start_time": "2021-12-30T09:24:56.482194Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T09:47:44.437944Z",
     "start_time": "2021-12-28T09:47:44.408949Z"
    }
   },
   "source": [
    "#  Features Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T10:35:09.397427Z",
     "start_time": "2021-12-28T10:35:09.102845Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to C:\\Users\\nir\n",
      "[nltk_data]     son\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import words\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T14:31:13.952861Z",
     "start_time": "2021-12-28T14:31:13.687725Z"
    }
   },
   "outputs": [],
   "source": [
    " ps = PorterStemmer()\n",
    "dictionary = dict(zip(words.words(), range(len(words.words()))))\n",
    "def toIndexVector(text: str, vector_len: int = 150) -> np.array:\n",
    "    vector = []\n",
    "    for word in nltk.word_tokenize(text):\n",
    "        stemed = ps.stem(word.lower())\n",
    "        try:\n",
    "            vector.append(dictionary[stemed])\n",
    "        except:\n",
    "            vector.append(-1)\n",
    "        \n",
    "    if(len(vector) < vector_len):\n",
    "            vector = vector + [0]*(vector_len - len(vector))\n",
    "    if(len(vector) > vector_len):\n",
    "            vector = vector[:vector_len]\n",
    "            \n",
    "    return pd.Series(vector)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T14:31:55.492149Z",
     "start_time": "2021-12-28T14:31:14.906687Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nir son\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\swifter\\swifter.py:37: UserWarning: This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\n",
      "  \"This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a1fa4c8348544c8b9c4097ef942b67a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=4244.0, style=ProgressStyle(descriptioâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "086326bf217f4e26b8c96f3c1c266626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=9901.0, style=ProgressStyle(descriptioâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x_train_index = x_train.copy(deep = True)\n",
    "x_test_index = x_test.copy(deep = True)\n",
    "x_test_index = pd.DataFrame(x_test_index['book_text'].swifter.apply(toIndexVector))\n",
    "x_train_index = pd.DataFrame(x_train_index['book_text'].swifter.apply(toIndexVector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T10:15:21.602899Z",
     "start_time": "2021-12-28T10:15:21.449269Z"
    }
   },
   "source": [
    "## bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __stem_text(text: str):\n",
    "    ps = PorterStemmer()\n",
    "    return (ps.stem(word) for word in word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T10:57:39.954806Z",
     "start_time": "2021-12-28T10:57:39.102459Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=0.003).fit(x_train['book_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T10:57:41.183710Z",
     "start_time": "2021-12-28T10:57:39.955810Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train_bag = pd.DataFrame(vectorizer.transform(x_train['book_text']).toarray())\n",
    "x_test_bag = pd.DataFrame(vectorizer.transform(x_test['book_text']).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T10:57:41.214831Z",
     "start_time": "2021-12-28T10:57:41.184634Z"
    }
   },
   "source": [
    "## word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T13:19:43.294151Z",
     "start_time": "2021-12-28T13:19:42.884390Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T13:24:39.953844Z",
     "start_time": "2021-12-28T13:24:05.850241Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nir son\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(400000, 50)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_output_file = '../glove.6B'+'.word2vec'\n",
    "glove2word2vec('../glove.6B/glove.6B.50d.txt', word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T13:25:13.702964Z",
     "start_time": "2021-12-28T13:24:51.836684Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T14:47:19.221196Z",
     "start_time": "2021-12-28T14:47:19.209227Z"
    }
   },
   "outputs": [],
   "source": [
    " ps = PorterStemmer()\n",
    "def word2vecDoc(text: str, doc_len: int = 100):\n",
    "    vec = []\n",
    "    for word in nltk.word_tokenize(text):\n",
    "        try:\n",
    "            we = model.get_vector(ps.stem(word.lower()))\n",
    "        except:\n",
    "            we = np.zeros(50)\n",
    "        \n",
    "        vec = np.concatenate([vec, we])\n",
    "    \n",
    "    if len(vec) < doc_len*50:\n",
    "        vec = np.concatenate([vec, np.zeros(doc_len*50 - len(vec))])\n",
    "    if len(vec) > doc_len*50:\n",
    "        vec = vec[: doc_len*50]\n",
    "    return pd.Series(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T14:48:19.491787Z",
     "start_time": "2021-12-28T14:47:20.097785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa4e308641d4ce7bceb9c943e020f09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=9901.0, style=ProgressStyle(descriptioâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ebe16ca1a244b618cfd38eda277c662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=4244.0, style=ProgressStyle(descriptioâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x_train_word2vec = pd.DataFrame(x_train['book_text'].swifter.apply(word2vecDoc))\n",
    "x_test_word2vec = pd.DataFrame(x_test['book_text'].swifter.apply(word2vecDoc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T14:53:45.852632Z",
     "start_time": "2021-12-28T14:52:54.755930Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim import models\n",
    "word2vec_path = '../GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T14:55:07.731604Z",
     "start_time": "2021-12-28T14:55:07.711661Z"
    }
   },
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "def doc2vec(text: str):\n",
    "    vec = np.zeros(300)\n",
    "    for word in nltk.word_tokenize(text):\n",
    "        try:\n",
    "            we = word2vec.get_vector(ps.stem(word.lower()))\n",
    "        except:\n",
    "            we = np.zeros(300)\n",
    "        \n",
    "        vec = vec + we\n",
    "    return pd.Series(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T14:55:54.793422Z",
     "start_time": "2021-12-28T14:55:08.305135Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a3057ab5744bf3b9960d26b5242b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=9901.0, style=ProgressStyle(descriptioâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5f7f4cf9342408a843e7553c1dc61b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=4244.0, style=ProgressStyle(descriptioâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x_train_doc2vec = pd.DataFrame(x_train['book_text'].swifter.apply(doc2vec))\n",
    "x_test_doc2vec = pd.DataFrame(x_test['book_text'].swifter.apply(doc2vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-30T09:36:11.381039Z",
     "start_time": "2021-12-30T09:36:11.338115Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\nir\n",
      "[nltk_data]     son\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "TEXT_COLUMN_LABEL = 'book_text'\n",
    "class FeaturesExtraction:\n",
    "    @staticmethod\n",
    "    def __stem_text(text: str):\n",
    "        ps = PorterStemmer()\n",
    "        return (ps.stem(word) for word in word_tokenize(text))\n",
    "\n",
    "    @staticmethod\n",
    "    def avg_word_len(df: pd.DataFrame, text_column_label: str = 'book_text') -> pd.DataFrame:\n",
    "        avg_word_len = df[text_column_label].astype(str).swifter.apply(\n",
    "            lambda s: pd.Series(nltk.word_tokenize(s)).map(len).mean()).rename(\"avg_word_len\")\n",
    "        return pd.DataFrame(avg_word_len)\n",
    "\n",
    "    @staticmethod\n",
    "    def avg_sentence_len(df: pd.DataFrame, text_column_label: str = 'book_text') -> pd.DataFrame:\n",
    "        sentence_count = df[text_column_label].astype(str).swifter.apply(\n",
    "            lambda text: pd.Series(nltk.sent_tokenize(text)).map(\n",
    "                lambda sent: len(nltk.word_tokenize(sent))).mean()).rename(\"avg_sentence_len\")\n",
    "\n",
    "        return pd.DataFrame(sentence_count)\n",
    "\n",
    "    @staticmethod\n",
    "    def punctuation_marks(df: pd.DataFrame, text_column_label: str = 'book_text') -> pd.DataFrame:\n",
    "        to_return = pd.DataFrame()\n",
    "        i = 1\n",
    "        for mark in list(string.punctuation):\n",
    "            to_return[f'punctuation{i}'] = df[text_column_label].astype(str).apply(lambda s: s.count(mark) / len(s))\n",
    "            i += 1\n",
    "        return to_return\n",
    "\n",
    "    @staticmethod\n",
    "    def stop_words(df: pd.DataFrame, text_column_label: str = 'book_text') -> pd.DataFrame:\n",
    "        to_return = pd.DataFrame()\n",
    "        for word in list(stopwords.words('english')):\n",
    "            to_return[word] = df[text_column_label].astype(str).apply(lambda s: s.count(word) / len(s))\n",
    "        return to_return\n",
    "\n",
    "    @staticmethod\n",
    "    def pos_count(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        def group_pos(tag):\n",
    "            groups = {\"noun\": ['NN', 'NNS', 'NNP', 'NNPS'], \"verb\": ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'],\n",
    "                      \"adverb\": ['RB', 'RBR', 'RBS'], \"adjective\": ['JJ', 'JJR', 'JJS']}\n",
    "            for key, group in groups.items():\n",
    "                if tag in group:\n",
    "                    return key\n",
    "            return None\n",
    "\n",
    "        features = df[TEXT_COLUMN_LABEL].astype(str).swifter.apply(\n",
    "            lambda s: pd.Series([x[1] for x in nltk.pos_tag(nltk.word_tokenize(s))]).\n",
    "                apply(group_pos).value_counts(normalize=True).copy())\n",
    "        features = features.fillna(0)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-30T09:36:18.448791Z",
     "start_time": "2021-12-30T09:36:18.444765Z"
    }
   },
   "outputs": [],
   "source": [
    "def simple_style(x) -> pd.DataFrame:\n",
    "    x = pd.concat(\n",
    "        [FeaturesExtraction.pos_count(x),\n",
    "         FeaturesExtraction.stop_words(x),\n",
    "         FeaturesExtraction.avg_word_len(x),\n",
    "         FeaturesExtraction.avg_sentence_len(x),\n",
    "         FeaturesExtraction.punctuation_marks(x)], axis=1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-30T09:38:50.727035Z",
     "start_time": "2021-12-30T09:36:23.173599Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nir son\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\swifter\\swifter.py:37: UserWarning: This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\n",
      "  \"This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff70785fff54a439a28b1a270e8413d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=9901.0, style=ProgressStyle(descriptioâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d39c9a4209c442aa074bb4ac27f2db7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=9901.0, style=ProgressStyle(descriptioâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "158cf4e7260d4d958b14e85e81956ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=9901.0, style=ProgressStyle(descriptioâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2fce035af004a1cb2aba4622aeabd62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=4244.0, style=ProgressStyle(descriptioâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4b4424db1944ffca44d698b7381ab4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=4244.0, style=ProgressStyle(descriptioâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c4f2088ec0245caa5e40521b5fc5d63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=4244.0, style=ProgressStyle(descriptioâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x_train_simple_style = simple_style(x_train)\n",
    "x_test_simple_style = simple_style(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-30T09:34:34.961481Z",
     "start_time": "2021-12-30T09:34:34.567963Z"
    }
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-30T09:34:35.259452Z",
     "start_time": "2021-12-30T09:34:35.239974Z"
    }
   },
   "outputs": [],
   "source": [
    "def pipeline(x_train: pd.DataFrame, x_test: pd.DataFrame):\n",
    "    model = xgb.XGBClassifier(use_label_encoder=False)\n",
    "    model.fit(x_train, y_train)\n",
    "    pre = model.predict(x_test)\n",
    "    print('accuracy = {}'.format((pre == y_test).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T17:31:21.375177Z",
     "start_time": "2021-12-28T17:29:30.789093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:29:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "accuracy = 0.08953817153628653\n"
     ]
    }
   ],
   "source": [
    "pipeline(x_train_index, x_test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T18:53:03.339662Z",
     "start_time": "2021-12-28T17:53:28.746022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:53:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "accuracy = 0.6979264844486334\n"
     ]
    }
   ],
   "source": [
    "pipeline(x_train_bag, x_test_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T22:16:57.161348Z",
     "start_time": "2021-12-28T18:53:03.353624Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:56:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "accuracy = 0.18355325164938738\n"
     ]
    }
   ],
   "source": [
    "pipeline(x_train_word2vec, x_test_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T17:42:42.804430Z",
     "start_time": "2021-12-28T17:32:37.517409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:32:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "accuracy = 0.5351083883129123\n"
     ]
    }
   ],
   "source": [
    "pipeline(x_train_doc2vec, x_test_doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-30T09:40:35.295100Z",
     "start_time": "2021-12-30T09:38:50.728034Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:38:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "accuracy = 0.339066918001885\n"
     ]
    }
   ],
   "source": [
    "pipeline(x_train_simple_style, x_test_simple_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T18:40:29.874980Z",
     "start_time": "2022-02-02T18:40:29.856808Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 6, 9])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([[1,2,3],[1,2,3],[1,2,3]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
