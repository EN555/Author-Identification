{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18a586b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T18:48:24.169443Z",
     "start_time": "2022-03-05T18:48:10.778247Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from PythonCode.preprocess.preprocess import *\n",
    "from PythonCode.preprocess.complexStyleFeatures import *\n",
    "from PythonCode.preprocess.simpleStyleFeatures import *\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "import pickle \n",
    "import gensim.downloader\n",
    "from pprint import pprint\n",
    "DATA_PATH_TRAIN = \"../Data/C50/C50train/\"\n",
    "DATA_PATH_TEST = \"../Data/C50/C50test/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea28c2f0",
   "metadata": {},
   "source": [
    "# Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f371de9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T18:48:24.900197Z",
     "start_time": "2022-03-05T18:48:24.206450Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_name</th>\n",
       "      <th>book_name</th>\n",
       "      <th>book_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>106247newsML.txt</td>\n",
       "      <td>The Internet may be overflowing with new techn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>120600newsML.txt</td>\n",
       "      <td>The U.S. Postal Service announced Wednesday a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>120683newsML.txt</td>\n",
       "      <td>Elementary school students with access to the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>136958newsML.txt</td>\n",
       "      <td>An influential Internet organisation has backe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>137498newsML.txt</td>\n",
       "      <td>An influential Internet organisation has backe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>WilliamKazer</td>\n",
       "      <td>28223newsML.txt</td>\n",
       "      <td>China's central bank chief has said that infla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>WilliamKazer</td>\n",
       "      <td>282935newsML.txt</td>\n",
       "      <td>China ushered in 1997, a year it has hailed as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>WilliamKazer</td>\n",
       "      <td>287736newsML.txt</td>\n",
       "      <td>China issued tough new rules on the handling o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>WilliamKazer</td>\n",
       "      <td>289747newsML.txt</td>\n",
       "      <td>China will avoid bold moves in tackling its ai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>WilliamKazer</td>\n",
       "      <td>304402newsML.txt</td>\n",
       "      <td>Communist Party chief Jiang Zemin has put his ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        author_name         book_name  \\\n",
       "0     AaronPressman  106247newsML.txt   \n",
       "1     AaronPressman  120600newsML.txt   \n",
       "2     AaronPressman  120683newsML.txt   \n",
       "3     AaronPressman  136958newsML.txt   \n",
       "4     AaronPressman  137498newsML.txt   \n",
       "...             ...               ...   \n",
       "2495   WilliamKazer   28223newsML.txt   \n",
       "2496   WilliamKazer  282935newsML.txt   \n",
       "2497   WilliamKazer  287736newsML.txt   \n",
       "2498   WilliamKazer  289747newsML.txt   \n",
       "2499   WilliamKazer  304402newsML.txt   \n",
       "\n",
       "                                              book_text  \n",
       "0     The Internet may be overflowing with new techn...  \n",
       "1     The U.S. Postal Service announced Wednesday a ...  \n",
       "2     Elementary school students with access to the ...  \n",
       "3     An influential Internet organisation has backe...  \n",
       "4     An influential Internet organisation has backe...  \n",
       "...                                                 ...  \n",
       "2495  China's central bank chief has said that infla...  \n",
       "2496  China ushered in 1997, a year it has hailed as...  \n",
       "2497  China issued tough new rules on the handling o...  \n",
       "2498  China will avoid bold moves in tackling its ai...  \n",
       "2499  Communist Party chief Jiang Zemin has put his ...  \n",
       "\n",
       "[2500 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = load_data(\"../Data/C50/C50train\",50)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4349846b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T18:54:26.075456Z",
     "start_time": "2022-03-05T18:54:05.983151Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_name</th>\n",
       "      <th>book_name</th>\n",
       "      <th>book_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>421829newsML.txt</td>\n",
       "      <td>U.S. Senators on Tuesday sharply criticized a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>424074newsML.txt</td>\n",
       "      <td>Two members of Congress criticised the Federal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>42764newsML.txt</td>\n",
       "      <td>Commuters stuck in traffic on the Leesburg Pik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>43033newsML.txt</td>\n",
       "      <td>A broad coalition of corporations went to Capi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>433558newsML.txt</td>\n",
       "      <td>On the Internet, where new products come and g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>WilliamKazer</td>\n",
       "      <td>504283newsML.txt</td>\n",
       "      <td>China has scored new successes in its fight ag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>WilliamKazer</td>\n",
       "      <td>504526newsML.txt</td>\n",
       "      <td>China has scored new successes in its fight ag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>WilliamKazer</td>\n",
       "      <td>51502newsML.txt</td>\n",
       "      <td>China is on target with plans to to promote 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>WilliamKazer</td>\n",
       "      <td>522090newsML.txt</td>\n",
       "      <td>China may need to adjust the mix of its treasu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>WilliamKazer</td>\n",
       "      <td>58312newsML.txt</td>\n",
       "      <td>A Chinese ideologue known for his strictly ort...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        author_name         book_name  \\\n",
       "0     AaronPressman  421829newsML.txt   \n",
       "1     AaronPressman  424074newsML.txt   \n",
       "2     AaronPressman   42764newsML.txt   \n",
       "3     AaronPressman   43033newsML.txt   \n",
       "4     AaronPressman  433558newsML.txt   \n",
       "...             ...               ...   \n",
       "2495   WilliamKazer  504283newsML.txt   \n",
       "2496   WilliamKazer  504526newsML.txt   \n",
       "2497   WilliamKazer   51502newsML.txt   \n",
       "2498   WilliamKazer  522090newsML.txt   \n",
       "2499   WilliamKazer   58312newsML.txt   \n",
       "\n",
       "                                              book_text  \n",
       "0     U.S. Senators on Tuesday sharply criticized a ...  \n",
       "1     Two members of Congress criticised the Federal...  \n",
       "2     Commuters stuck in traffic on the Leesburg Pik...  \n",
       "3     A broad coalition of corporations went to Capi...  \n",
       "4     On the Internet, where new products come and g...  \n",
       "...                                                 ...  \n",
       "2495  China has scored new successes in its fight ag...  \n",
       "2496  China has scored new successes in its fight ag...  \n",
       "2497  China is on target with plans to to promote 10...  \n",
       "2498  China may need to adjust the mix of its treasu...  \n",
       "2499  A Chinese ideologue known for his strictly ort...  \n",
       "\n",
       "[2500 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = load_data(\"../Data/C50/C50test\",50)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21ad82aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T18:55:39.952320Z",
     "start_time": "2022-03-05T18:55:39.933314Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df_train.append(df_test, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68f922c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T18:56:55.468961Z",
     "start_time": "2022-03-05T18:56:55.453970Z"
    }
   },
   "outputs": [],
   "source": [
    "TEST_PART = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe63378",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4320bf51",
   "metadata": {},
   "source": [
    "### Pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1254cefe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T18:57:28.730252Z",
     "start_time": "2022-03-05T18:57:28.704268Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(df[\"book_text\"],df[\"author_name\"],test_size=TEST_PART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4eeb6331",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T19:15:25.463398Z",
     "start_time": "2022-03-05T19:15:25.441423Z"
    }
   },
   "outputs": [],
   "source": [
    "def avg_word_len(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    avg_word_len = df.astype(str).swifter.apply(\n",
    "        lambda s: pd.Series(nltk.word_tokenize(s)).map(len).mean()).rename(\"avg_word_len\")\n",
    "    return pd.DataFrame(avg_word_len)\n",
    "\n",
    "\n",
    "def avg_sentence_len(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    sentence_count = df.astype(str).swifter.apply(\n",
    "        lambda text: pd.Series(nltk.sent_tokenize(text)).map(\n",
    "            lambda sent: len(nltk.word_tokenize(sent))).mean()).rename(\"avg_sentence_len\")\n",
    "\n",
    "    return pd.DataFrame(sentence_count)\n",
    "\n",
    "def hapax_disLegemena(x: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    hapax disLegemena Measure for lexicographic diversity: v2/N\n",
    "    v2 = number of words that appears twice\n",
    "    N = number of words or number of unique words\n",
    "    \"\"\"\n",
    "\n",
    "    def get_hapax_disLegemena(text: str) -> (float, float):\n",
    "        words = nltk.word_tokenize(text)\n",
    "        words_count = Counter(words)\n",
    "        v2 = len([_ for _, count in words_count.items() if count == 2])\n",
    "        return pd.Series((v2 / (len(words) + EPSILON), v2 / (len(set(words))+EPSILON)))\n",
    "\n",
    "    return pd.DataFrame(x.astype(str).swifter.apply(get_hapax_disLegemena)).set_axis(['hapax_disLegemena(H)', 'hapax_disLegemena(S)'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90271ee4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T19:17:42.804988Z",
     "start_time": "2022-03-05T19:15:26.002387Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5250eaaad0d47b18c1d3f63337c99f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/4500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d0a87c926e4585b812d352414e5482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/4500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d67c48ca94b04d2398a29fbff33ffa32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/4500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e28461d4496c413fb6baad72d86c2e12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc46ed669b1d4bcaa37194d4300255b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a9350305924344abada82172ff399f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess(x_train: pd.DataFrame, x_test: pd.DataFrame) -> (pd.DataFrame, pd.DataFrame):\n",
    "    def helper(x: pd.DataFrame) -> pd.DataFrame:\n",
    "        return pd.concat([avg_word_len(x),avg_sentence_len(x),hapax_disLegemena(x)], axis=1)\n",
    "        \n",
    "    return helper(x_train), helper(x_test)\n",
    "\n",
    "X_train,X_test = preprocess(X_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4fa4a354",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T19:18:14.087190Z",
     "start_time": "2022-03-05T19:18:14.072176Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train = pd.Categorical(y_train).codes\n",
    "y_test = pd.Categorical(y_test).codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1353d27d",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c06fba97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T19:21:51.158137Z",
     "start_time": "2022-03-05T19:19:17.823549Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(max_depth=7, random_state=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier(n_estimators=100,max_depth=7, random_state=0)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89940c0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T19:22:05.657283Z",
     "start_time": "2022-03-05T19:22:05.475290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.17      0.40      0.24         5\n",
      "           1       0.30      0.23      0.26        13\n",
      "           2       0.00      0.00      0.00         5\n",
      "           3       0.00      0.00      0.00        10\n",
      "           4       0.09      0.11      0.10         9\n",
      "           5       0.00      0.00      0.00         8\n",
      "           6       0.10      0.09      0.10        11\n",
      "           7       0.16      0.23      0.19        13\n",
      "           8       0.22      0.20      0.21        10\n",
      "           9       0.12      0.12      0.12         8\n",
      "          10       0.07      0.14      0.09         7\n",
      "          11       0.00      0.00      0.00        10\n",
      "          12       0.14      0.08      0.11        12\n",
      "          13       0.06      0.20      0.09         5\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.38      0.23      0.29        13\n",
      "          16       0.10      0.07      0.08        14\n",
      "          17       0.08      0.17      0.11         6\n",
      "          18       0.11      0.08      0.09        13\n",
      "          19       0.18      0.25      0.21         8\n",
      "          20       0.00      0.00      0.00        10\n",
      "          21       0.07      0.12      0.09         8\n",
      "          22       0.14      0.09      0.11        11\n",
      "          23       0.00      0.00      0.00        13\n",
      "          24       0.00      0.00      0.00        11\n",
      "          25       0.12      0.20      0.15         5\n",
      "          26       0.33      0.40      0.36         5\n",
      "          27       0.10      0.11      0.11         9\n",
      "          28       0.08      0.11      0.09         9\n",
      "          29       0.11      0.09      0.10        11\n",
      "          30       0.00      0.00      0.00         5\n",
      "          31       0.14      0.09      0.11        11\n",
      "          32       0.33      0.33      0.33        12\n",
      "          33       0.00      0.00      0.00         8\n",
      "          34       0.08      0.17      0.11         6\n",
      "          35       0.08      0.12      0.10         8\n",
      "          36       0.29      0.27      0.28        15\n",
      "          37       0.17      0.06      0.09        17\n",
      "          38       0.00      0.00      0.00        13\n",
      "          39       0.12      0.08      0.10        13\n",
      "          40       0.00      0.00      0.00         8\n",
      "          41       0.50      0.45      0.48        11\n",
      "          42       0.00      0.00      0.00         6\n",
      "          43       0.00      0.00      0.00        14\n",
      "          44       0.00      0.00      0.00        10\n",
      "          45       0.15      0.17      0.16        12\n",
      "          46       0.21      0.20      0.21        15\n",
      "          47       0.15      0.14      0.15        14\n",
      "          48       0.06      0.08      0.07        12\n",
      "          49       0.00      0.00      0.00         9\n",
      "\n",
      "    accuracy                           0.12       500\n",
      "   macro avg       0.11      0.12      0.11       500\n",
      "weighted avg       0.12      0.12      0.11       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f16b47",
   "metadata": {},
   "source": [
    "# Using Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01c80ebb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T18:58:27.432510Z",
     "start_time": "2022-03-05T18:58:26.956504Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ed45fbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T19:27:24.405407Z",
     "start_time": "2022-03-05T19:25:51.498331Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
     ]
    }
   ],
   "source": [
    "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c1ec0ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T19:28:44.508254Z",
     "start_time": "2022-03-05T19:28:44.490259Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glove_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "8f5a98e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T09:42:15.703935Z",
     "start_time": "2022-03-07T09:42:15.694230Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "# stemmer = nltk.stem.PorterStemmer()\n",
    "def tranform_word(word:str) -> np.ndarray:\n",
    "    word = re.sub(r'[^a-z]', '', word.lower())\n",
    "    if word in glove_vectors:\n",
    "        return glove_vectors[word]\n",
    "#     if not simple:\n",
    "#         token = lemmatizer.lemmatize(word)\n",
    "#         if token in glove_vectors:\n",
    "#             return glove_vectors[token]\n",
    "#         token = stemmer.stem(word)\n",
    "#         if token in glove_vectors:\n",
    "#             return glove_vectors[token]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "0714ee30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T10:15:12.294564Z",
     "start_time": "2022-03-07T10:15:12.279571Z"
    }
   },
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10a32c8",
   "metadata": {},
   "source": [
    "## Sentence Level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4c89c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T06:53:31.292627Z",
     "start_time": "2022-03-07T06:53:31.287615Z"
    }
   },
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "539ecfe7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T09:46:32.581097Z",
     "start_time": "2022-03-07T09:46:32.561097Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(df[\"book_text\"],df[\"author_name\"],test_size=TEST_PART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "c2cfea13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T09:46:32.914464Z",
     "start_time": "2022-03-07T09:46:32.904439Z"
    }
   },
   "outputs": [],
   "source": [
    "def num_sentences_based_chucking(df: pd.DataFrame, chunk_size: int):\n",
    "    rows = []\n",
    "    for row in df:\n",
    "        sentences = nltk.tokenize.sent_tokenize(row)\n",
    "        curr_chunk = []\n",
    "        for sentence in sentences:\n",
    "            curr_chunk.append(sentence)\n",
    "            if len(curr_chunk) == chunk_size:\n",
    "                rows.append(\"\".join(curr_chunk))\n",
    "                curr_chunk = []\n",
    "        \n",
    "        rows.append(\"\".join(curr_chunk))  # add the last one\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "54f73e7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T09:46:36.737177Z",
     "start_time": "2022-03-07T09:46:33.812097Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35535,), (3934,))"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_OF_SENTENCE_CHUNK = 3\n",
    "X_train = num_sentences_based_chucking(X_train,NUM_OF_SENTENCE_CHUNK)[0]\n",
    "X_test = num_sentences_based_chucking(X_test,NUM_OF_SENTENCE_CHUNK)[0]\n",
    "X_train.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81a9d1e",
   "metadata": {},
   "source": [
    "#### Decide max number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "92557027",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T07:05:43.943386Z",
     "start_time": "2022-03-07T07:05:33.404461Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f256c50bd372498a99429a56a27f1410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/35525 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\nanops.py:1675: DeprecationWarning: the `interpolation=` argument to percentile was renamed to `method=`, which has additional options.\n",
      "Users of the modes 'nearest', 'lower', 'higher', or 'midpoint' are encouraged to review the method they. (Deprecated NumPy 1.22)\n",
      "  return np.percentile(values, q, axis=axis, interpolation=interpolation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    35525.000000\n",
       "mean        70.791893\n",
       "std         28.567110\n",
       "min          0.000000\n",
       "25%         56.000000\n",
       "50%         73.000000\n",
       "75%         90.000000\n",
       "max        307.000000\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words = X_train.swifter.apply(lambda text: len(nltk.word_tokenize(text)))\n",
    "num_words.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "45a5206d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T07:06:48.639289Z",
     "start_time": "2022-03-07T07:06:48.619294Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\nanops.py:1675: DeprecationWarning: the `interpolation=` argument to percentile was renamed to `method=`, which has additional options.\n",
      "Users of the modes 'nearest', 'lower', 'higher', or 'midpoint' are encouraged to review the method they. (Deprecated NumPy 1.22)\n",
      "  return np.percentile(values, q, axis=axis, interpolation=interpolation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.95    113.00\n",
       "0.97    118.00\n",
       "0.98    122.52\n",
       "0.99    128.00\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words.quantile([0.95,0.97,0.98,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "bb278eb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T09:17:47.584168Z",
     "start_time": "2022-03-07T09:17:47.571173Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1353     179\n",
       "15730    307\n",
       "21318    195\n",
       "33340    209\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words[num_words>170]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "421739c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T09:18:15.760602Z",
     "start_time": "2022-03-07T09:18:15.748605Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 170"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b55fbe",
   "metadata": {},
   "source": [
    "#### Continue Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "fa4f8dac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T09:57:25.079272Z",
     "start_time": "2022-03-07T09:57:25.065274Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad_matrix(arr:np.ndarray,max_length:int) -> np.ndarray:\n",
    "    if arr.size == 0:\n",
    "        return None\n",
    "    if arr.shape[0] == max_length:\n",
    "        return arr\n",
    "    if arr.shape[0] > max_length:\n",
    "        return arr[:max_length,:]\n",
    "    return np.concatenate([arr,np.zeros((max_length - arr.shape[0],arr.shape[1]))],axis=0,dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "97c412c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T10:07:37.260021Z",
     "start_time": "2022-03-07T10:07:37.245024Z"
    }
   },
   "outputs": [],
   "source": [
    "def sentence_level_preprocess(text:str):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    result = []\n",
    "    missing_embedding_count = 0\n",
    "    for word in words:\n",
    "        embedding = tranform_word(word)\n",
    "        if embedding is not None:\n",
    "            result.append(embedding)\n",
    "        else:\n",
    "            missing_embedding_count+=1\n",
    "    return pd.Series({\"data\": pad_matrix(np.array(result),MAX_LENGTH),\"missing_embedding_count\":missing_embedding_count})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "8ea30161",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T09:57:30.053732Z",
     "start_time": "2022-03-07T09:57:26.456927Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f985e77098a24fedb893bf777b5794ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/3934 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = X_test.swifter.apply(sentence_level_preprocess)\n",
    "# res['missing_embedding_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "3c8493a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T10:17:50.147472Z",
     "start_time": "2022-03-07T10:17:50.129448Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(X):\n",
    "#     X_train = num_sentences_based_chucking(X_train,NUM_OF_SENTENCE_CHUNK)[0]\n",
    "    res = X.swifter.apply(sentence_level_preprocess)[\"data\"].dropna().reset_index(drop=True)\n",
    "    return np.vstack(res).reshape((res.size,MAX_LENGTH,EMBEDDING_SIZE))\n",
    "#X_train,X_test,y_train,y_test = train_test_split(df[\"book_text\"],df[\"author_name\"],test_size=TEST_PART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "4b1fa958",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T10:18:38.736254Z",
     "start_time": "2022-03-07T10:18:01.795564Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64cafa99e7144776a7d191c6a5bcadd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/3934 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f6e4c3094be4560bfa39bafd1a0f1c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/35535 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_test = preprocess(X_test)\n",
    "X_train = preprocess(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebea712",
   "metadata": {},
   "source": [
    "**Note that we tried to stemm or lemmatize the word before search in glove table but it doesnt help at all**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "2b38217d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T10:18:40.703799Z",
     "start_time": "2022-03-07T10:18:40.697806Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34049, 170, 50)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6501cf8",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "4b1447b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T10:22:43.532039Z",
     "start_time": "2022-03-07T10:22:43.423031Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_5 (GRU)                  (None, 100)               45600     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50)                5050      \n",
      "=================================================================\n",
      "Total params: 50,650\n",
      "Trainable params: 50,650\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense,GRU,AvgPool2D\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(GRU(100,recurrent_dropout=0.2,input_shape=(MAX_LENGTH,EMBEDDING_SIZE)))\n",
    "model.add(AvgPool2D((1,50)))\n",
    "model.add(Dense(50, activation=\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer='adam',metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f203f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=15)\n",
    "history = model.fit(x=X_train, y=y_train, epochs=200, shuffle=True,\n",
    "          batch_size=200, validation_data=(X_test, y_test),callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21b78ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"sentence_level\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60737a05",
   "metadata": {},
   "source": [
    "## Article Level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afc381b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T06:53:55.171287Z",
     "start_time": "2022-03-07T06:53:55.157291Z"
    }
   },
   "source": [
    "### Preprocess\n",
    "**Problems:**\n",
    "1. tranform words without glove embedding\n",
    "2. unequal setence length(padding required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "6bc76359",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T10:42:20.230585Z",
     "start_time": "2022-03-07T10:42:20.220588Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(df[\"book_text\"],df[\"author_name\"],test_size=TEST_PART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc584ea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "bc22765f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T10:42:20.754315Z",
     "start_time": "2022-03-07T10:42:20.737313Z"
    }
   },
   "outputs": [],
   "source": [
    "def article_level_preprocess(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    result = []\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        curr_result = []\n",
    "        for word in words:\n",
    "            embedding = tranform_word(word)\n",
    "            if embedding is not None:\n",
    "                curr_result.append(embedding)\n",
    "        if len(curr_result) != 0:\n",
    "            result.append(np.array(curr_result,dtype=float).mean(axis=1,dtype=float))\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "81b7ce31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T10:42:24.653965Z",
     "start_time": "2022-03-07T10:42:21.369320Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed897800a48d471d8f01b327584b5d65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yosef\\AppData\\Local\\Temp\\ipykernel_11000\\3053542387.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      [[-0.014834297150373458, 0.05241892080754042, ...\n",
       "1      [[-0.05828151840716601, -0.028394063198938967,...\n",
       "2      [[0.04884604448452592, -0.19339120510034263, -...\n",
       "3      [[-0.08446934077132028, 0.06881276426836848, 0...\n",
       "4      [[0.10178176319226623, -0.047792823422933, 0.0...\n",
       "                             ...                        \n",
       "495    [[-0.03665994189679623, 0.01958741317735985, -...\n",
       "496    [[-0.1340274625271559, 0.05241892080754042, 0....\n",
       "497    [[-0.07820000560721382, 0.15220197901129723, 0...\n",
       "498    [[-0.010296121351420879, -0.010455437414348125...\n",
       "499    [[-0.013328241184353828, -0.05217864386737347,...\n",
       "Name: book_text, Length: 500, dtype: object"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.swifter.apply(article_level_preprocess).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "a628f294",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T10:42:52.085404Z",
     "start_time": "2022-03-07T10:42:52.079402Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29,)"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899545e7",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f414669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense,GRU,AvgPool2D\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "model = Sequential()\n",
    "model.add(GRU(10,recurrent_dropout=0.5),input_shape=(100,1)))\n",
    "model.add(AvgPool2D())\n",
    "model.add(Dense(50, activation=\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer='adam',metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc33a610",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=15)\n",
    "history = model.fit(x=X_train, y=y_train, epochs=200, shuffle=True,\n",
    "          batch_size=200, validation_data=(X_test, y_test),callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c49d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"article_level\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
