{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e9cd938",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T12:47:48.681931Z",
     "start_time": "2022-04-10T12:47:43.127728Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from PythonCode.preprocess.common import *\n",
    "from PythonCode.preprocess.complexStyleFeatures import *\n",
    "from PythonCode.preprocess.simpleStyleFeatures import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b4e7c41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T12:47:59.652292Z",
     "start_time": "2022-04-10T12:47:48.714806Z"
    }
   },
   "outputs": [],
   "source": [
    "df = load_data(\"../Data/C50/C50test\", 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce131408",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T12:47:59.700181Z",
     "start_time": "2022-04-10T12:47:59.686172Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_name</th>\n",
       "      <th>book_name</th>\n",
       "      <th>book_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>421829newsML.txt</td>\n",
       "      <td>U.S. Senators on Tuesday sharply criticized a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>424074newsML.txt</td>\n",
       "      <td>Two members of Congress criticised the Federal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>42764newsML.txt</td>\n",
       "      <td>Commuters stuck in traffic on the Leesburg Pik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>43033newsML.txt</td>\n",
       "      <td>A broad coalition of corporations went to Capi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>433558newsML.txt</td>\n",
       "      <td>On the Internet, where new products come and g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>WilliamKazer</td>\n",
       "      <td>504283newsML.txt</td>\n",
       "      <td>China has scored new successes in its fight ag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>WilliamKazer</td>\n",
       "      <td>504526newsML.txt</td>\n",
       "      <td>China has scored new successes in its fight ag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>WilliamKazer</td>\n",
       "      <td>51502newsML.txt</td>\n",
       "      <td>China is on target with plans to to promote 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>WilliamKazer</td>\n",
       "      <td>522090newsML.txt</td>\n",
       "      <td>China may need to adjust the mix of its treasu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>WilliamKazer</td>\n",
       "      <td>58312newsML.txt</td>\n",
       "      <td>A Chinese ideologue known for his strictly ort...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        author_name         book_name  \\\n",
       "0     AaronPressman  421829newsML.txt   \n",
       "1     AaronPressman  424074newsML.txt   \n",
       "2     AaronPressman   42764newsML.txt   \n",
       "3     AaronPressman   43033newsML.txt   \n",
       "4     AaronPressman  433558newsML.txt   \n",
       "...             ...               ...   \n",
       "2495   WilliamKazer  504283newsML.txt   \n",
       "2496   WilliamKazer  504526newsML.txt   \n",
       "2497   WilliamKazer   51502newsML.txt   \n",
       "2498   WilliamKazer  522090newsML.txt   \n",
       "2499   WilliamKazer   58312newsML.txt   \n",
       "\n",
       "                                              book_text  \n",
       "0     U.S. Senators on Tuesday sharply criticized a ...  \n",
       "1     Two members of Congress criticised the Federal...  \n",
       "2     Commuters stuck in traffic on the Leesburg Pik...  \n",
       "3     A broad coalition of corporations went to Capi...  \n",
       "4     On the Internet, where new products come and g...  \n",
       "...                                                 ...  \n",
       "2495  China has scored new successes in its fight ag...  \n",
       "2496  China has scored new successes in its fight ag...  \n",
       "2497  China is on target with plans to to promote 10...  \n",
       "2498  China may need to adjust the mix of its treasu...  \n",
       "2499  A Chinese ideologue known for his strictly ort...  \n",
       "\n",
       "[2500 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "155d0863",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T18:51:05.557103Z",
     "start_time": "2022-04-02T18:49:07.401146Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bfa0c70dfdb42228fb1c1c7337c3767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83770d14fa5e4f0da9f22ac3826604f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05488c717c0645eb8e03850532fd764e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ba7f1f03af7442ca10d5433a14fcff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "693aff7541944904a2ce8eaacc58c6f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/502M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = TFBertModel.from_pretrained(\"bert-base-cased\")\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='tf')\n",
    "output = model(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd38ce72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T13:15:14.791170Z",
     "start_time": "2022-04-10T13:15:08.955454Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m1\u001b[39m)(x)  \u001b[38;5;66;03m# Linear model\u001b[39;00m\n\u001b[0;32m      9\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m outputs \u001b[38;5;241m=\u001b[39m forward_pass(\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mModel(inputs, outputs)\n\u001b[0;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mBinaryCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mpreprocess\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess\u001b[39m(x):\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtext_vectorizer\u001b[49m(x)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text_vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def preprocess(x):\n",
    "    return text_vectorizer(x)\n",
    "\n",
    "def forward_pass(x):\n",
    "    return tf.keras.layers.Dense(1)(x)  # Linear model\n",
    "\n",
    "inputs = tf.keras.Input(shape=(1,), dtype='string')\n",
    "outputs = forward_pass(preprocess(inputs))\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True))\n",
    "model.fit(train_ds, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca16616d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T13:37:35.896389Z",
     "start_time": "2022-04-10T13:37:35.485965Z"
    }
   },
   "outputs": [],
   "source": [
    "df = merge_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e98f447",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T13:38:47.353296Z",
     "start_time": "2022-04-10T13:38:47.344239Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df[TEXT_COLUMN_NAME].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525480af",
   "metadata": {},
   "source": [
    "<!-- https://huggingface.co/dslim/bert-base-NER -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4490ab71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T13:57:45.500969Z",
     "start_time": "2022-04-10T13:57:45.017226Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "num_sentences_based_chucking() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39msummary())\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m---> 61\u001b[0m \u001b[43msentence_level_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerge_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36msentence_level_pipeline\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Dense(\u001b[38;5;241m50\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m)(x)\n\u001b[0;32m     49\u001b[0m X \u001b[38;5;241m=\u001b[39m df[TEXT_COLUMN_NAME]\n\u001b[1;32m---> 50\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mnum_sentences_based_chucking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_OF_SENTENCE_CHUNK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m], data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m], test_size\u001b[38;5;241m=\u001b[39mTEST_PART)\n\u001b[0;32m     52\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: num_sentences_based_chucking() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from src.config.Constants import *\n",
    "from keras.layers import Dense, GRU, AvgPool1D, Masking\n",
    "\n",
    "from src.preprocess.common import merge_datasets, num_sentences_based_chucking\n",
    "from src.preprocess.word_embedding_features import sentence_level_preprocess\n",
    "\n",
    "DATA_PATH = \"../data/C50\"\n",
    "\n",
    "\n",
    "def article_level_pipeline():\n",
    "    def forward_pass(x):\n",
    "        x = Masking(mask_value=0., input_shape=(MAX_NUMBER_OF_SENTENCE, MAX_SENTENCE_LENGTH))(x)\n",
    "        x = GRU(100, recurrent_dropout=0.2, return_sequences=True)(x)\n",
    "        x = AvgPool1D(pool_size=(MAX_NUMBER_OF_SENTENCE,))(x)\n",
    "        return Dense(50, activation=\"softmax\")(x)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[TEXT_COLUMN_NAME], df[AUTHOR_NAME_COLUMN_NAME],\n",
    "                                                        test_size=TEST_PART)\n",
    "    inputs = tf.keras.Input(shape=(1,), dtype='string')\n",
    "    outputs = forward_pass(sentence_level_preprocess(inputs))\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "    print(model.summary())\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=VALIDATION_PART)\n",
    "\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30)\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=f\"./article-level-checkpoints\",\n",
    "                                                                   save_weights_only=False,\n",
    "                                                                   monitor='val_accuracy', mode='max',\n",
    "                                                                   save_best_only=True)\n",
    "    history = model.fit(x=X_train, y=y_train, epochs=30000, shuffle=True,\n",
    "                        batch_size=200, validation_data=(X_val, y_val), callbacks=[callback, model_checkpoint_callback])\n",
    "    with open(\"article-level-history\", \"w\") as file:\n",
    "        pickle.dump(history, file)\n",
    "    model.save(\"article-level\")\n",
    "\n",
    "\n",
    "def sentence_level_pipeline(df: pd.DataFrame):\n",
    "    def forward_pass(x):\n",
    "        x = Masking(mask_value=0., input_shape=(MAX_LENGTH, EMBEDDING_SIZE))(x)\n",
    "        x = GRU(100, recurrent_dropout=0.2, return_sequences=True)(x)\n",
    "        x = AvgPool1D(pool_size=(170,))(x)\n",
    "        return Dense(50, activation=\"softmax\")(x)\n",
    "\n",
    "    X = df[TEXT_COLUMN_NAME]\n",
    "    data = num_sentences_based_chucking(X, NUM_OF_SENTENCE_CHUNK, \"X\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[\"X\"], data[\"y\"], test_size=TEST_PART)\n",
    "    inputs = tf.keras.Input(shape=(1,), dtype='string')\n",
    "    outputs = forward_pass(sentence_level_preprocess(inputs))\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True))\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "sentence_level_pipeline(merge_datasets())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d48282d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
